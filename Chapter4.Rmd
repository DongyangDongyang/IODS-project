---
---
---

# 4:Clustering and Classification

-   Describe your work and results clearly.
-   Assume the reader has an introductory course level understanding of writing and reading R code as well as statistical methods.
-   Assume the reader has no previous knowledge of your data or the more advanced methods you are using.

## describe the data (Task 2)
```{r}
library(MASS)

# load the data 
data("Boston")

# explore the dataset # 506 observations and 14 variables. The data type is mostly numeric except "chas" and "rad" are integer.
str(Boston)
dim(Boston)
```

## relationship between variables (Task 3)
```{r}

library(MASS)
library(tidyr)
library(corrplot)

# graphical overview of the data
pairs(Boston)

# calculate the correlation matrix and round it
cor_matrix <- cor(Boston) 

# print the correlation matrix
cor_matrix

# adjust the code to round the matrix
round_cor_matrix <- cor(Boston) %>% round(digits = 2)
round_cor_matrix

# visualize the correlation matrix
library(corrplot)
corrplot(cor_matrix, method="circle", cl.pos = "b", tl.pos = "d", tl.cex = 0.6)
# Variable "age" and "dis", "nox" and "dis", "indus" and "dis", "medv" and "lstat" has a strong negative relationship. Variable "rad" and "tax", "indus" and "tax" had a strong positive relationship. 


# summaries of variables
summary(Boston)
# These statistics provides insights into the central tendency, spread, and distribution of each variable in the data set. The minimum, maximum, mean, and quartiles for each variable also can be seen. 

```

## standardize the dataset (Taks 4)
```{r}
library(MASS)
data("Boston")

# center and standardize variables
boston_scaled <- scale(Boston)

# summaries of the scaled variables
summary(boston_scaled)
# The data become more central. The data follows a normal distribution.

# class of the boston_scaled object
class(boston_scaled)

# change the object to data frame
boston_scaled <- as.data.frame(boston_scaled)

# change the type of data
boston_scaled$crim <- as.numeric(boston_scaled$crim)

# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE)

# adjust the code
label <- c("low", "med_low", "med_high", "high")
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = label)

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset
boston_scaled <- dplyr::select(boston_scaled, -crim)

# add the new categorical value to scaled data
boston_scaled <- data.frame(boston_scaled, crime)

# number of rows in the Boston dataset 
n <- nrow(boston_scaled)

# choose randomly 80% of the rows
ind <- sample(n,  size = n * 0.8)

# create train set
train <- boston_scaled[ind,]

# create test set 
test <- boston_scaled[-ind,]

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
```

## Linear discriminant analysis on the train set (Task 5)
```{r}
library(MASS)
boston_scaled <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/boston_scaled.txt",
                            sep=",", header = T)
boston_scaled$crime <- factor(boston_scaled$crime, levels = c("low", "med_low", "med_high", "high"))
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)

# linear discriminant analysis
lda.fit <- lda(crime~., data = train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  graphics::arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results (select both lines and execute them at the same time!)
plot(lda.fit, dimen = 2, col = classes, pch = classes)
lda.arrows(lda.fit, myscale = 1)

```

## Predict the classes with the LDA model on the test data (Task 6)
```{r}
library(MASS)
boston_scaled <- read.table("https://raw.githubusercontent.com/KimmoVehkalahti/Helsinki-Open-Data-Science/master/datasets/boston_scaled.txt",
                            sep=",", header = T)
boston_scaled$crime <- factor(boston_scaled$crime, levels = c("low", "med_low", "med_high", "high"))
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)
train <- boston_scaled[ind,]
test <- boston_scaled[-ind,]
correct_classes <- test$crime
test <- dplyr::select(test, -crime)
lda.fit = lda(crime ~ ., data=train)

# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
table(correct = correct_classes, predicted = lda.pred$class)
# Each row represents the actual classes, and each column represents the predicted classes. 73.53% of predictions are correct in general.

```

## Visualize the clusters (Taks 7)
```{r}

library(MASS)
data("Boston")

# Load necessary libraries
library(MASS)
library(cluster)
library(ggplot2)

# Reload the Boston dataset
data("Boston")

# Standardize the dataset
boston_scaled <- scale(Boston)

# Calculate the distances between observations
distances <- dist(boston_scaled)

# Determine the optimal number of clusters using the elbow method
wss <- numeric(10)
for (k in 1:10) {
  wss[k] <- sum(kmeans(boston_scaled, centers = k)$withinss)
}

# Plot the within-cluster sum of squares (WSS) against the number of clusters
plot(1:10, wss, type = "b", xlab = "Number of Clusters (k)", ylab = "Within-cluster Sum of Squares (WSS)",
     main = "Elbow Method for Optimal k")
# The "elbow" in the plot occurs for five groups, try them out and decided better k-means is 3.

# Try: Run k-means algorithm with the 3 of clusters
kmeans_result <- kmeans(boston_scaled, centers = 3)

# Set the maximum number of clusters
set.seed(123)

# determine the number of clusters
k_max <- 10

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(Boston, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

# k-means clustering
km <- kmeans(Boston, centers = 2)

# plot the Boston data set with clusters
pairs(Boston, col = km$cluster)

# the total WCSS, the optimal number of clusters is when the total WCSS drops radically, which is 2 in this data set. Hence, the data set can be divided into two clusters. Points within the same cluster exhibit similar pattern and the plot demonstrates the correlations between variables. 
```
